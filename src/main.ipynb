{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxHS9LEN4dpA",
        "outputId": "6fa5430b-692c-4fb4-fe61-1bc1792cd028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "unzipping...\n",
            "sample loaded: 500\n",
            "cleaning & extracting skills...\n",
            "clustering...\n",
            "cluster\n",
            "1    355\n",
            "0     72\n",
            "3     19\n",
            "7     16\n",
            "4     12\n",
            "6      9\n",
            "5      9\n",
            "2      8\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Salary vs skills... (if salary exists)\n",
            "\n",
            "Avg salary per cluster:\n",
            "cluster\n",
            "1    90756.716447\n",
            "6    84400.000000\n",
            "4    83620.000000\n",
            "3    79630.590909\n",
            "0    79111.789474\n",
            "2    46220.000000\n",
            "7    31356.250000\n",
            "Name: normalized_salary, dtype: float64\n",
            "\n",
            "Cluster 4 (avg 83620.0):\n",
            "  cloud: 4\n",
            "  go: 2\n",
            "\n",
            "Cluster 1 (avg 90756.71644736842):\n",
            "  excel: 1\n",
            "  power bi: 1\n",
            "  linux: 1\n",
            "  agile: 1\n",
            "  spark: 1\n",
            "\n",
            "Cluster 0 (avg 79111.78947368421):\n",
            "  excel: 38\n",
            "\n",
            "Cluster 7 (avg 31356.25):\n",
            "  javascript: 1\n",
            "  php: 1\n",
            "  aws: 1\n",
            "  backend: 1\n",
            "\n",
            "Cluster 2 (avg 46220.0):\n",
            "  excel: 3\n",
            "  go: 3\n",
            "\n",
            "Cluster 3 (avg 79630.59090909091):\n",
            "  go: 11\n",
            "\n",
            "Cluster 6 (avg 84400.0):\n",
            "  sql: 2\n",
            "  java: 1\n",
            "done.\n"
          ]
        }
      ],
      "source": [
        "import os, re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from google.colab import drive\n",
        "\n",
        "# settings\n",
        "ZIP_FILE_PATH = '/content/drive/MyDrive/linkedin/linkedin.zip'\n",
        "UNZIP_DIR = '/content/linkedin_unzipped'\n",
        "POSTINGS_FILE = f'{UNZIP_DIR}/postings.csv'\n",
        "SAMPLE_SIZE = 500\n",
        "\n",
        "# basic list of skills (not exhaustive)\n",
        "TECH_SKILLS = [\n",
        "    'python','java','sql','aws','tensorflow','machine learning','data science',\n",
        "    'javascript','react','node.js','c++','c#','php','ruby','go','swift','kotlin',\n",
        "    'docker','kubernetes','azure','gcp','cloud','linux','unix','git','tableau',\n",
        "    'power bi','excel','spark','hadoop','scala','r','agile','scrum','jira',\n",
        "    'confluence','api','frontend','backend','fullstack','devops','cybersecurity'\n",
        "]\n",
        "\n",
        "NORMALIZATION_MAP = {\n",
        "    'ml':'machine learning','ds':'data science','js':'javascript','nodejs':'node.js',\n",
        "    'cpp':'c++','csharp':'c#','golang':'go','powerbi':'power bi'\n",
        "}\n",
        "\n",
        "\n",
        "def setup_nltk():\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt_tab/english.pickle')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt_tab')\n",
        "    try:\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    if not os.path.exists(UNZIP_DIR):\n",
        "        os.makedirs(UNZIP_DIR)\n",
        "\n",
        "    print(\"unzipping...\")\n",
        "    !unzip -qqo \"{ZIP_FILE_PATH}\" -d \"{UNZIP_DIR}\"\n",
        "\n",
        "    df = pd.read_csv(POSTINGS_FILE)\n",
        "    df = df.head(SAMPLE_SIZE).copy()  # small sample for speed\n",
        "\n",
        "    print(\"sample loaded:\", len(df))\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    t = text.lower()\n",
        "    t = re.sub(r'http\\S+|www\\S+', '', t)\n",
        "    t = re.sub(r'[^a-z\\s]', ' ', t)\n",
        "\n",
        "    tokens = word_tokenize(t)\n",
        "    sw = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if w not in sw]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def find_skills(t):\n",
        "    out = []\n",
        "    for s in TECH_SKILLS:\n",
        "        if re.search(r'\\b'+re.escape(s)+r'\\b', t):\n",
        "            out.append(s)\n",
        "    return list(set(out))\n",
        "\n",
        "\n",
        "def normalize(sk_list):\n",
        "    out = []\n",
        "    for s in sk_list:\n",
        "        s2 = NORMALIZATION_MAP.get(s.lower(), s.lower())\n",
        "        out.append(s2)\n",
        "    return sorted(list(set(out)))\n",
        "\n",
        "\n",
        "def extract_skills(df):\n",
        "    print(\"cleaning & extracting skills...\")\n",
        "\n",
        "    df['clean_text'] = df['description'].apply(clean)\n",
        "    df['skills'] = df['clean_text'].apply(find_skills)\n",
        "    df['skills'] = df['skills'].apply(normalize)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def cluster_jobs(df, k=8):\n",
        "    print(\"clustering...\")\n",
        "\n",
        "    df['skills_str'] = df['skills'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    vec = TfidfVectorizer(max_features=100)\n",
        "    X = vec.fit_transform(df['skills_str'])\n",
        "\n",
        "    model = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "    df['cluster'] = model.fit_predict(X)\n",
        "\n",
        "    print(df['cluster'].value_counts())\n",
        "    return df\n",
        "\n",
        "\n",
        "def analyze_salary(df):\n",
        "    print(\"\\nSalary vs skills... (if salary exists)\")\n",
        "\n",
        "    if 'normalized_salary' not in df.columns:\n",
        "        print(\"no salary field found\")\n",
        "        return\n",
        "\n",
        "    s = df[df['normalized_salary'].notnull()]\n",
        "    if s.empty:\n",
        "        print(\"no salary data in sample\")\n",
        "        return\n",
        "\n",
        "    avg = s.groupby('cluster')['normalized_salary'].mean().sort_values(ascending=False)\n",
        "    print(\"\\nAvg salary per cluster:\")\n",
        "    print(avg)\n",
        "\n",
        "    # top skills\n",
        "    for c in s['cluster'].unique():\n",
        "        subset = s[s['cluster']==c]\n",
        "        skills = [x for lst in subset['skills'] for x in lst]\n",
        "        counts = Counter(skills).most_common(5)\n",
        "        print(f\"\\nCluster {c} (avg {avg.get(c,'?')}):\")\n",
        "        for skill, cnt in counts:\n",
        "            print(f\"  {skill}: {cnt}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    setup_nltk()\n",
        "    df = load_data()\n",
        "    df = extract_skills(df)\n",
        "    df = cluster_jobs(df, 8)\n",
        "    analyze_salary(df)\n",
        "    print(\"done.\")"
      ]
    }
  ]
}